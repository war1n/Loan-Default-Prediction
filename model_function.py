import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import scipy.stats as stats
from sklearn.preprocessing import OneHotEncoder
from sklearn.model_selection import cross_val_score
from sklearn.svm import SVC
from xgboost import XGBClassifier
from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc
from sklearn.metrics import roc_auc_score, make_scorer

from sklearn.model_selection import GridSearchCV

from sklearn.model_selection import cross_val_score, StratifiedKFold
from sklearn.metrics import make_scorer, fbeta_score
from imblearn.over_sampling import SMOTE
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier


from sklearn.feature_selection import SelectFromModel
from sklearn.model_selection import cross_val_score, StratifiedKFold
from sklearn.metrics import make_scorer, fbeta_score
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from imblearn.over_sampling import SMOTE
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer

import math

def summarize_columns(df, target_col):
    """
    ‡∏™‡∏£‡∏∏‡∏õ‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡πÉ‡∏ô DataFrame
    - ‡πÅ‡∏¢‡∏Å‡πÄ‡∏õ‡πá‡∏ô numeric, categorical ‡πÅ‡∏•‡∏∞ target

    Parameters:
    df (pd.DataFrame): DataFrame ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå
    target_col (str): ‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô Target

    Returns:
    dict: {'numeric': [num_cols], 'categorical': [cat_cols], 'target': target_col}
    """
    num_cols = df.select_dtypes(include=['number']).columns.tolist()
    cat_cols = df.select_dtypes(include=['object']).columns.tolist()
    
    if target_col in num_cols:
        num_cols.remove(target_col)
    elif target_col in cat_cols:
        cat_cols.remove(target_col)

    return {"numeric": num_cols, "categorical": cat_cols, "target": target_col}

def handle_missing_values(df, plot_missing=False):
    """
    ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Å‡∏±‡∏ö Missing Values ‡πÅ‡∏•‡∏∞‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ plot ‡∏õ‡∏£‡∏¥‡∏°‡∏≤‡∏ì Missing Value (%)
    - ‡∏ñ‡πâ‡∏≤ column ‡∏°‡∏µ missing ‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤ 20% ‡πÉ‡∏´‡πâ‡∏î‡∏£‡∏≠‡∏õ column ‡∏ô‡∏±‡πâ‡∏ô
    - ‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô column ‡πÄ‡∏•‡∏Ç (numeric) ‡πÅ‡∏•‡∏∞‡∏°‡∏µ missing ‡∏ô‡πâ‡∏≠‡∏¢‡∏Å‡∏ß‡πà‡∏≤ 20% ‡πÉ‡∏´‡πâ fill ‡∏î‡πâ‡∏ß‡∏¢ median
    - ‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô column categorical ‡πÉ‡∏´‡πâ fill ‡∏î‡πâ‡∏ß‡∏¢ mode
    
    Parameters:
    df (pd.DataFrame): DataFrame ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Å‡∏±‡∏ö missing values
    plot_missing (bool): ‡∏´‡∏≤‡∏Å‡πÄ‡∏õ‡πá‡∏ô True ‡∏à‡∏∞‡πÅ‡∏™‡∏î‡∏á‡∏Å‡∏£‡∏≤‡∏ü missing value
    
    Returns:
    pd.DataFrame: DataFrame ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Å‡∏±‡∏ö missing values
    """
    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏≠‡∏±‡∏ï‡∏£‡∏≤‡∏Å‡∏≤‡∏£ missing ‡πÉ‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞ column
    missing_percentage = df.isnull().mean() * 100
    
    if plot_missing:
        plt.figure(figsize=(10, 5))
        missing_percentage[missing_percentage > 0].sort_values().plot(kind='barh', color='skyblue')
        plt.xlabel('Missing Value (%)')
        plt.ylabel('Columns')
        plt.title('Missing Value Percentage per Column')
        plt.grid(axis='x', linestyle='--', alpha=0.7)
        plt.show()
    
    # ‡∏•‡∏ö‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏°‡∏µ missing ‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤ 20%
    columns_to_drop = missing_percentage[missing_percentage > 20].index
    df = df.drop(columns=columns_to_drop)

    # ‡πÅ‡∏ö‡πà‡∏á‡πÄ‡∏õ‡πá‡∏ô numeric ‡∏Å‡∏±‡∏ö categorical columns
    numeric_cols = df.select_dtypes(include=['number']).columns
    categorical_cols = df.select_dtypes(include=['object']).columns

    # Fill missing values ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö numeric columns ‡∏î‡πâ‡∏ß‡∏¢ median
    for col in numeric_cols:
        df[col] = df[col].fillna(df[col].median())

    # Fill missing values ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö categorical columns ‡∏î‡πâ‡∏ß‡∏¢ mode
    for col in categorical_cols:
        if not df[col].mode().empty:
            df[col] = df[col].fillna(df[col].mode()[0])

    return df

def cap_outliers(df, ignore_feature=None, plot=False):
    """
    Cap outliers in any numeric column using the 1.5 * IQR rule, excluding the target_feature.
    Optionally plot boxplots before and after capping for affected columns.
    
    Parameters:
        df (pd.DataFrame): Input DataFrame.
        ignore_feature (str, optional): Column to be ignored when capping outliers.
        plot (bool): Whether to plot boxplots before and after capping.
    
    Returns:
        pd.DataFrame: DataFrame with outliers capped.
    """
    df = df.copy()
    affected_columns = []
    original_data = df.copy()
    
    for col in df.select_dtypes(include=['int64', 'float64']).columns:
        if col == ignore_feature:
            continue
        
        Q1 = df[col].quantile(0.25)
        Q3 = df[col].quantile(0.75)
        IQR = Q3 - Q1
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR
        
        if df[(df[col] < lower_bound) | (df[col] > upper_bound)].shape[0] > 0:
            affected_columns.append(col)
            
        df[col] = np.where(df[col] < lower_bound, lower_bound, df[col])
        df[col] = np.where(df[col] > upper_bound, upper_bound, df[col])
    
    if plot and affected_columns:
        num_cols = len(affected_columns)
        fig, axes = plt.subplots(num_cols, 2, figsize=(10, 5 * num_cols))
        
        if num_cols == 1:
            axes = [axes]
        
        for ax, col in zip(axes, affected_columns):
            ax[0].boxplot(original_data[col], vert=False)
            ax[0].set_title(f"{col} Before Capping")
            
            ax[1].boxplot(df[col], vert=False)
            ax[1].set_title(f"{col} After Capping")
        
        plt.tight_layout()
        plt.show()
    
    return df

def explore_data(df):
    """
    Explore the dataset by visualizing missing values, numerical distributions, and categorical distributions.
    
    Parameters:
        df (pd.DataFrame): Input DataFrame.
    """
    def plot_missing_values(df):
        missing_percentage = df.isnull().mean() * 100
        missing_percentage = missing_percentage[missing_percentage > 0].sort_values(ascending=False)
        
        if not missing_percentage.empty:
            plt.figure(figsize=(10, 5))
            sns.barplot(x=missing_percentage.index, y=missing_percentage.values, color='navy')
            plt.xticks(rotation=90)
            plt.ylabel("% Missing Values")
            plt.title("Missing Values Percentage by Column")
            plt.show()
        else:
            print("No missing values in the dataset.")
    
    def plot_numeric_features(df):
        numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns
        
        for col in numeric_cols:
            fig, axes = plt.subplots(1, 2, figsize=(12, 4))
            sns.boxplot(x=df[col], ax=axes[0])
            axes[0].set_title(f"Boxplot of {col}")
            
            sns.histplot(df[col], kde=True, ax=axes[1])
            axes[1].set_title(f"Distribution of {col}")
            
            plt.tight_layout()
            plt.show()
    
    def plot_categorical_features(df):
        categorical_cols = df.select_dtypes(exclude=['int64', 'float64']).columns
        num_cats = len(categorical_cols)
        
        if num_cats > 0:
            rows = (num_cats // 4) + (num_cats % 4 > 0)
            fig, axes = plt.subplots(rows, 4, figsize=(16, 4 * rows))
            axes = axes.flatten()
            
            for i, col in enumerate(categorical_cols):
                sns.countplot(x=df[col], order=df[col].value_counts().index, ax=axes[i], color='navy')
                axes[i].set_title(f"Countplot of {col}")
                axes[i].tick_params(axis='x', rotation=90)
            
            for j in range(i + 1, len(axes)):
                fig.delaxes(axes[j])
            
            plt.tight_layout()
            plt.show()
        else:
            print("No categorical features in the dataset.")
    
    print("Exploring Missing Values:")
    plot_missing_values(df)
    print("\nExploring Numeric Features:")
    plot_numeric_features(df)
    print("\nExploring Categorical Features:")
    plot_categorical_features(df)


def plot_feature_relationships(df, target_col):
    """
    ‡∏ß‡∏≤‡∏î‡∏Å‡∏£‡∏≤‡∏ü‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á features ‡πÅ‡∏•‡∏∞ target
    - Numeric: Distribution Plot
    - Categorical: Bar Chart

    Parameters:
    df (pd.DataFrame): DataFrame ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå
    target_col (str): ‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô Target
    """
    summary = summarize_columns(df, target_col)
    
    num_cols = summary['numeric']
    cat_cols = summary['categorical']

    # ‡∏™‡∏£‡πâ‡∏≤‡∏á subplot ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Numeric Features
    if num_cols:
        n = len(num_cols)
        ncols = 3  # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡πÉ‡∏´‡πâ‡∏°‡∏µ 3 ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå
        nrows = math.ceil(n / ncols)  # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÅ‡∏ñ‡∏ß‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£
        fig, axes = plt.subplots(nrows, ncols, figsize=(ncols*6, nrows*4))  # ‡∏Ç‡∏ô‡∏≤‡∏î‡∏Ç‡∏≠‡∏á‡∏†‡∏≤‡∏û
        axes = axes.flatten()  # ‡∏ó‡∏≥‡πÉ‡∏´‡πâ axes ‡πÄ‡∏õ‡πá‡∏ô list ‡πÅ‡∏ó‡∏ô‡∏ó‡∏µ‡πà‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô array 2D
        for i, col in enumerate(num_cols):
            sns.kdeplot(df[df[target_col] == 0][col], label="Non-Default", shade=True, color="blue", ax=axes[i])
            sns.kdeplot(df[df[target_col] == 1][col], label="Default", shade=True, color="red", ax=axes[i])
            axes[i].set_title(f"Distribution of {col}")
            axes[i].legend()
        # ‡∏õ‡∏¥‡∏î subplot ‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏û‡∏≠
        for j in range(i + 1, len(axes)):
            axes[j].axis('off')
        plt.tight_layout()  # ‡∏ó‡∏≥‡πÉ‡∏´‡πâ spacing ‡∏Ç‡∏≠‡∏á subplot ‡πÑ‡∏°‡πà‡∏ó‡∏±‡∏ö‡∏Å‡∏±‡∏ô
        plt.show()

    # Plot Bar Chart ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Categorical Features
    hue_colors = {1: '#CB3335', 0: '#477CA8'}
    if cat_cols:
        n = len(cat_cols)
        ncols = 6  # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡πÉ‡∏´‡πâ‡∏°‡∏µ 6 ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå
        nrows = math.ceil(n / ncols)  # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÅ‡∏ñ‡∏ß‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£
        fig, axes = plt.subplots(nrows, ncols, figsize=(ncols*6, nrows*4))  # ‡∏Ç‡∏ô‡∏≤‡∏î‡∏Ç‡∏≠‡∏á‡∏†‡∏≤‡∏û
        axes = axes.flatten()  # ‡∏ó‡∏≥‡πÉ‡∏´‡πâ axes ‡πÄ‡∏õ‡πá‡∏ô list ‡πÅ‡∏ó‡∏ô‡∏ó‡∏µ‡πà‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô array 2D
        for i, col in enumerate(cat_cols):
            sns.countplot(x=col, hue=target_col, data=df, palette=hue_colors, ax=axes[i])
            axes[i].set_title(f"Count Plot of {col} by {target_col}")
            axes[i].set_xticklabels(axes[i].get_xticklabels(), rotation=45)
        # ‡∏õ‡∏¥‡∏î subplot ‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏û‡∏≠
        for j in range(i + 1, len(axes)):
            axes[j].axis('off')
        plt.tight_layout()  # ‡∏ó‡∏≥‡πÉ‡∏´‡πâ spacing ‡∏Ç‡∏≠‡∏á subplot ‡πÑ‡∏°‡πà‡∏ó‡∏±‡∏ö‡∏Å‡∏±‡∏ô
        plt.show()

def plot_correlation_heatmap(df, target_col):
    """
    ‡πÅ‡∏™‡∏î‡∏á Heatmap ‡∏Ç‡∏≠‡∏á correlation ‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£ numeric ‡∏Å‡∏±‡∏ö Target

    Parameters:
    df (pd.DataFrame): DataFrame ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå
    target_col (str): ‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô Target
    """
    summary = summarize_columns(df, target_col)
    num_cols = summary['numeric'] + [target_col]

    corr_matrix = df[num_cols].corr()

    plt.figure(figsize=(8,6))
    sns.heatmap(corr_matrix, annot=True, cmap="coolwarm", fmt=".2f", linewidths=0.5)
    plt.title("Correlation Heatmap")
    plt.show()

def chi_square_test(df, target_col):
    """
    ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Chi-Square Test ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏î‡∏π‡∏ß‡πà‡∏≤ categorical variable ‡πÉ‡∏î‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå‡∏Å‡∏±‡∏ö target

    Parameters:
    df (pd.DataFrame): DataFrame ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå
    target_col (str): ‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô Target

    Returns:
    pd.DataFrame: ‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡πÅ‡∏™‡∏î‡∏á p-value ‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞ categorical variable
    """
    summary = summarize_columns(df, target_col)
    cat_cols = summary['categorical']

    chi2_results = {}
    alpha = 0.05  # ‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏ô‡∏±‡∏¢‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç

    for col in cat_cols:
        contingency_table = pd.crosstab(df[col], df[target_col])
        chi2, p, dof, expected = stats.chi2_contingency(contingency_table)
        chi2_results[col] = p

    chi2_df = pd.DataFrame.from_dict(chi2_results, orient='index', columns=['p-value']).sort_values(by='p-value')
    
    print("\nüîç Significant Variables:")
    print(chi2_df[chi2_df['p-value'] < alpha])

    return chi2_df


def encode_categorical_features(df, target_col):
    """
    ‡∏ó‡∏≥ One-Hot Encoding ‡∏Å‡∏±‡∏ö categorical variables ‡πÅ‡∏•‡∏∞‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô DataFrame

    Parameters:
    df (pd.DataFrame): DataFrame ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ encoding
    target_col (str): ‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô Target

    Returns:
    pd.DataFrame: DataFrame ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å encode ‡πÅ‡∏•‡πâ‡∏ß
    """
    summary = summarize_columns(df, target_col)
    cat_cols = summary['categorical']
    
    encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)  # ‚úÖ ‡πÉ‡∏ä‡πâ sparse_output=False ‡πÅ‡∏ó‡∏ô sparse=False
    encoded_cat = encoder.fit_transform(df[cat_cols])

    # ‡∏™‡∏£‡πâ‡∏≤‡∏á DataFrame ‡∏Ç‡∏≠‡∏á One-Hot Encoded Features
    encoded_df = pd.DataFrame(encoded_cat, columns=encoder.get_feature_names_out(cat_cols), index=df.index)

    # ‡∏£‡∏ß‡∏°‡∏Å‡∏±‡∏ö numeric columns ‡πÅ‡∏•‡∏∞ target
    final_df = pd.concat([df[summary['numeric']], encoded_df, df[target_col]], axis=1)

    return final_df



def create_model_pipeline(X_train, y_train, models_dict, cv_folds=5):
    """
    ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ ROC-AUC Score ‡πÅ‡∏•‡∏∞‡∏ó‡∏≥ Grid Search ‡∏´‡∏≤ hyperparameters ‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î

    Parameters:
    X_train (pd.DataFrame): ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå‡∏Ç‡∏≠‡∏á‡∏ä‡∏∏‡∏î training
    y_train (pd.Series): ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• target ‡∏Ç‡∏≠‡∏á‡∏ä‡∏∏‡∏î training
    models_dict (dict): ‡∏û‡∏à‡∏ô‡∏≤‡∏ô‡∏∏‡∏Å‡∏£‡∏°‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡πá‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÅ‡∏•‡∏∞‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞‡πÇ‡∏°‡πÄ‡∏î‡∏•
    cv_folds (int): ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô folds ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Cross-Validation (‡∏Ñ‡πà‡∏≤ default = 5)

    Returns:
    tuple: ‡∏ä‡∏∑‡πà‡∏≠‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î, ‡∏Ñ‡πà‡∏≤ ROC-AUC Score ‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î, pipeline ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ù‡∏∂‡∏Å‡πÅ‡∏•‡πâ‡∏ß
    """
    best_model = None
    best_score = 0
    results = {}

    # ‡πÉ‡∏ä‡πâ Stratified K-Fold
    cv = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=42)

    # ‡πÉ‡∏ä‡πâ ROC-AUC ‡πÄ‡∏õ‡πá‡∏ô metric
    roc_auc_scorer = make_scorer(roc_auc_score)

    # 1Ô∏è‚É£ ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î (Baseline Model)
    for name, model_info in models_dict.items():
        print(f'üîç Evaluating Model: {name} ...')

        model = model_info['model']

        # ‡πÉ‡∏ä‡πâ SMOTE ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÅ‡∏Å‡πâ‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• imbalance
        smote = SMOTE(sampling_strategy='auto', random_state=42)
        X_resampled, y_resampled = smote.fit_resample(X_train, y_train)

        # ‡∏™‡∏£‡πâ‡∏≤‡∏á Pipeline
        pipeline = Pipeline([
            ('scaler', StandardScaler()),
            ('classifier', model)
        ])

        # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡πà‡∏≤ ROC-AUC Score
        roc_auc_scores = cross_val_score(pipeline, X_resampled, y_resampled, cv=cv, scoring=roc_auc_scorer)
        mean_roc_auc = roc_auc_scores.mean()
        results[name] = mean_roc_auc

        print(f'ROC-AUC Score: {mean_roc_auc:.4f}')

        # ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î
        if mean_roc_auc > best_score:
            best_score = mean_roc_auc
            best_model = name

    print(f"\nüèÜ Best Model: {best_model} with ROC-AUC Score: {best_score:.4f}")

    # 2Ô∏è‚É£ ‡∏ó‡∏≥ Grid Search ‡∏Å‡∏±‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î
    best_model_info = models_dict[best_model]
    best_model_instance = best_model_info['model']
    param_grid = {f'classifier__{key}': value for key, value in best_model_info['params'].items()}

    print(f"\nüîç Performing Grid Search for {best_model} ...")

    # ‡∏™‡∏£‡πâ‡∏≤‡∏á Pipeline ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Grid Search
    pipeline = Pipeline([
        ('scaler', StandardScaler()),
        ('classifier', best_model_instance)
    ])

    # ‡πÉ‡∏ä‡πâ GridSearchCV ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏´‡∏≤‡∏Ñ‡πà‡∏≤ hyperparameter ‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î
    grid_search = GridSearchCV(pipeline, param_grid, cv=cv, scoring=roc_auc_scorer, n_jobs=-1)
    grid_search.fit(X_train, y_train)

    # ‡πÑ‡∏î‡πâ‡∏Ñ‡πà‡∏≤‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î
    best_pipeline = grid_search.best_estimator_
    best_params = grid_search.best_params_
    best_final_score = grid_search.best_score_

    print(f"‚úÖ Best Parameters for {best_model}: {best_params}")
    print(f"üèÜ Final ROC-AUC Score: {best_final_score:.4f}")

    return best_model, best_final_score, best_pipeline


# def create_model_pipeline(X_train, y_train, model=RandomForestClassifier(random_state=42), cv_folds=5, param_grid=None):
#     """
#     ‡∏™‡∏£‡πâ‡∏≤‡∏á Pipeline ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ó‡∏≥ Standardization, SMOTE, Feature Selection (SelectFromModel),
#     ‡∏Å‡∏≤‡∏£‡∏ó‡∏≥ GridSearch ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏´‡∏≤‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î ‡πÅ‡∏•‡∏∞ Cross-Validation ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì F2 Score

#     Parameters:
#     X_train (pd.DataFrame): DataFrame ‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡πá‡∏ö‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå‡πÉ‡∏ô‡∏ä‡∏∏‡∏î training
#     y_train (pd.Series): Series ‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡πá‡∏ö target ‡πÉ‡∏ô‡∏ä‡∏∏‡∏î training
#     model (sklearn model object): ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ (‡∏Ñ‡πà‡∏≤ default = RandomForestClassifier)
#     cv_folds (int): ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô folds ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Cross-Validation (‡∏Ñ‡πà‡∏≤ default = 5)
#     param_grid (dict): ‡∏û‡∏à‡∏ô‡∏≤‡∏ô‡∏∏‡∏Å‡∏£‡∏°‡∏Ç‡∏≠‡∏á‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏à‡∏≤‡∏Å Grid Search (‡∏Ñ‡πà‡∏≤ default = None)

#     Returns:
#     tuple: ‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢‡∏Ç‡∏≠‡∏á F2 Score ‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£‡∏ó‡∏≥ Cross-Validation ‡πÅ‡∏•‡∏∞ pipeline ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ù‡∏∂‡∏Å‡πÅ‡∏•‡πâ‡∏ß
#     """
#     # ‡∏™‡∏£‡πâ‡∏≤‡∏á Pipeline
#     smote = SMOTE(sampling_strategy='auto', random_state=42)
#     X_train, y_train = smote.fit_resample(X_train, y_train)

#     pipeline = Pipeline([
#         ('scaler', StandardScaler()),  # Standardize data
#         ('classifier', model)          # ‡πÉ‡∏™‡πà‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏•‡∏∑‡∏≠‡∏Å
#     ])
    
#     # ‡∏´‡∏≤‡∏Å‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏£‡∏∞‡∏ö‡∏∏ param_grid ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô
#     if param_grid is None:
#         param_grid = {
#             'classifier__n_estimators': [100, 200],
#             'classifier__max_depth': [None, 10, 20]
#         }
    
#     # ‡∏™‡∏£‡πâ‡∏≤‡∏á F2 Score Scorer
#     f2_scorer = make_scorer(fbeta_score, beta=2)
    
#     # ‡πÉ‡∏ä‡πâ StratifiedKFold ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏ï‡πà‡∏•‡∏∞ fold ‡∏°‡∏µ‡∏™‡∏±‡∏î‡∏™‡πà‡∏ß‡∏ô‡∏Ç‡∏≠‡∏á target ‡∏ó‡∏µ‡πà‡πÄ‡∏ó‡πà‡∏≤‡∏Å‡∏±‡∏ô
#     cv = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=42)

#     # ‡∏™‡∏£‡πâ‡∏≤‡∏á GridSearchCV
#     grid_search = GridSearchCV(pipeline, param_grid, cv=cv, scoring=f2_scorer, n_jobs=-1)
    
#     # ‡∏ó‡∏≥‡∏Å‡∏≤‡∏£ Grid Search ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏´‡∏≤‡∏Ñ‡πà‡∏≤‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î
#     grid_search.fit(X_train, y_train)

#     # ‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢‡∏Ç‡∏≠‡∏á F2 Score ‡∏à‡∏≤‡∏Å GridSearchCV
#     mean_f2_score = grid_search.best_score_
    
#     # ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
#     print(f"üîç F2 Score (‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£‡∏ó‡∏≥ {cv_folds}-fold cross-validation): {mean_f2_score:.4f}")
#     print(f"üîç ‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î: {grid_search.best_params_}")
    
#     # ‡∏ù‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏î‡πâ‡∏ß‡∏¢‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î
#     best_pipeline = grid_search.best_estimator_

#     return mean_f2_score, best_pipeline


def get_model_accuracy(model, params, X, y, cv_folds=5):
    # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î StratifiedKFold ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö cross-validation
    stratified_kfold = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=42)
    
    # F2 Score
    f2_scorer = make_scorer(fbeta_score, beta=2)  # ‡πÉ‡∏ä‡πâ F2-score

    # ‡∏™‡∏£‡πâ‡∏≤‡∏á GridSearchCV ‡∏î‡πâ‡∏ß‡∏¢ StratifiedKFold
    grid = GridSearchCV(model,
                        params,
                        scoring=f2_scorer,
                        cv=stratified_kfold,
                        error_score=0.)
    grid.fit(X, y)
    
    # ‡πÄ‡∏Å‡πá‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
    best_score = grid.best_score_
    best_params = grid.best_params_
    fit_time = round(grid.cv_results_['mean_fit_time'].mean(), 3)
    score_time = round(grid.cv_results_['mean_score_time'].mean(), 3)
    

    print('Best F2 Score: {:.4f}'.format(grid.best_score_))
    print('Best Parameters: {}'.format(grid.best_params_))
    print('Average Time of Fit (s): {:.3f}'.format(grid.cv_results_['mean_fit_time'].mean()))
    print('Average Time to Score (s): {:.3f}'.format(grid.cv_results_['mean_score_time'].mean()))
    
    # Return ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏£‡∏ß‡∏°‡∏ñ‡∏∂‡∏á grid object ‡∏î‡πâ‡∏ß‡∏¢
    return {
        'best_score': best_score,
        'best_params': best_params,
        'fit_time': fit_time,
        'score_time': score_time,
        'grid': grid
    }

def compare_models(models, params, X, y):
    results = []
    
    # ‡∏ß‡∏ô loop ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞ model ‡πÅ‡∏•‡∏∞‡πÄ‡∏Å‡πá‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
    for model_name, model in models.items():
        print(f"Running GridSearchCV for {model_name}...")
        result = get_model_accuracy(model, params[model_name], X, y)
        result['model'] = model_name  # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ä‡∏∑‡πà‡∏≠ model
        results.append(result)
    
    # ‡πÅ‡∏õ‡∏•‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡πÄ‡∏õ‡πá‡∏ô DataFrame ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏ó‡∏µ‡πà‡∏á‡πà‡∏≤‡∏¢‡∏Ç‡∏∂‡πâ‡∏ô
    results_df = pd.DataFrame(results)
    return results_df


def plot_boxplots_comparison(df1, df2, num_cols):
    """
    ‡∏ß‡∏≤‡∏î Boxplot ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á DataFrame 2 ‡∏ï‡∏±‡∏ß
    ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡πÉ‡∏ô num_cols
    
    Parameters:
    df1 (pd.DataFrame): DataFrame ‡∏ï‡∏±‡∏ß‡πÅ‡∏£‡∏Å
    df2 (pd.DataFrame): DataFrame ‡∏ï‡∏±‡∏ß‡∏ó‡∏µ‡πà‡∏™‡∏≠‡∏á
    num_cols (list): ‡∏£‡∏≤‡∏¢‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô Numeric ‡πÉ‡∏ô‡∏ó‡∏±‡πâ‡∏á‡∏™‡∏≠‡∏á DataFrame
    """
    n = len(num_cols)
    ncols = 4  # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡πÉ‡∏´‡πâ‡∏°‡∏µ 3 ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå
    nrows = math.ceil(n / ncols)  # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÅ‡∏ñ‡∏ß‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£
    
    fig, axes = plt.subplots(nrows, ncols, figsize=(ncols*6, nrows*4))  # ‡∏Ç‡∏ô‡∏≤‡∏î‡∏Ç‡∏≠‡∏á‡∏†‡∏≤‡∏û
    axes = axes.flatten()  # ‡∏ó‡∏≥‡πÉ‡∏´‡πâ axes ‡πÄ‡∏õ‡πá‡∏ô list ‡πÅ‡∏ó‡∏ô‡∏ó‡∏µ‡πà‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô array 2D
    
    for i, col in enumerate(num_cols):
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á DataFrame ‡πÉ‡∏´‡∏°‡πà‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏£‡∏∞‡∏ö‡∏∏‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡∏≠‡∏á df1 ‡∏´‡∏£‡∏∑‡∏≠ df2
        df1_copy = df1[[col]].copy()
        df1_copy['DataFrame'] = 'Before'
        
        df2_copy = df2[[col]].copy()
        df2_copy['DataFrame'] = 'After'
        
        # ‡∏£‡∏ß‡∏°‡∏ó‡∏±‡πâ‡∏á‡∏™‡∏≠‡∏á DataFrame
        combined_df = pd.concat([df1_copy, df2_copy])
        
        # ‡∏ß‡∏≤‡∏î Boxplot
        sns.boxplot(x='DataFrame', y=col, data=combined_df, ax=axes[i], palette="Set1")
        axes[i].set_title(f"Boxplot of {col}")
    
    # ‡∏õ‡∏¥‡∏î subplot ‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏û‡∏≠
    for j in range(i + 1, len(axes)):
        axes[j].axis('off')
    
    plt.tight_layout()  # ‡∏ó‡∏≥‡πÉ‡∏´‡πâ spacing ‡∏Ç‡∏≠‡∏á subplot ‡πÑ‡∏°‡πà‡∏ó‡∏±‡∏ö‡∏Å‡∏±‡∏ô
    plt.show()


def plot_status_comparison(status_series,name):
    """
    ‡∏™‡∏£‡πâ‡∏≤‡∏á bar chart ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ç‡∏≠‡∏á Status 1 ‡πÅ‡∏•‡∏∞ 0 ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ‡∏™‡∏µ‡∏ï‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î
    ‡πÅ‡∏•‡∏∞‡πÅ‡∏™‡∏î‡∏á‡πÄ‡∏õ‡∏≠‡∏£‡πå‡πÄ‡∏ã‡πá‡∏ô‡∏ï‡πå‡∏ö‡∏ô‡∏´‡∏±‡∏ß‡∏Ç‡∏≠‡∏á‡πÅ‡∏ó‡πà‡∏á‡πÉ‡∏ô bar chart
    
    Parameters:
    status_series (pd.Series): Series ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡∏≠‡∏á Status (1 ‡∏´‡∏£‡∏∑‡∏≠ 0)
    """
    # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏™‡∏µ
    hue_colors = {1: '#CB3335', 0: '#477CA8'}
    
    # ‡∏™‡∏£‡πâ‡∏≤‡∏á bar chart
    plt.figure(figsize=(6, 4))
    ax = sns.countplot(x=status_series, palette=hue_colors)

    # ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏Ñ‡πà‡∏≤‡πÉ‡∏ô x-axis
    ax.set_xticklabels(['Non-Default', 'Default'])

    # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡πÅ‡∏•‡∏∞‡πÅ‡∏™‡∏î‡∏á‡πÄ‡∏õ‡∏≠‡∏£‡πå‡πÄ‡∏ã‡πá‡∏ô‡∏ï‡πå‡∏ö‡∏ô‡∏´‡∏±‡∏ß‡πÅ‡∏ó‡πà‡∏á
    total = len(status_series)
    for p in ax.patches:
        height = p.get_height()
        percentage = (height / total) * 100
        ax.text(p.get_x() + p.get_width() / 2, height + 0.05, f'{percentage:.2f}%', 
                ha='center', va='bottom', fontsize=12)
    
    # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡πÉ‡∏ô‡∏Å‡∏£‡∏≤‡∏ü
    plt.title(f"Count Plot of Status of {name}")
    plt.xlabel('Status')
    plt.ylabel('Count')
    plt.tight_layout()
    plt.show()
